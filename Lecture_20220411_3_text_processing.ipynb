{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><center>Методы анализа текстов</center></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Этапы анализа текстов\n",
    "\n",
    "При обработке текстов выделяют несколько этапов анализа.\n",
    "* Токенизация (графематический анализ) - выделение абзацев, предложений, токенов. Если абзацы в HTML выделяются довольно просто - по тегам &lt;p&gt;, то с выделением предложений и слов могут быть проблемы.\n",
    "`Г. Мурманск был основан 3 апреля 1915 г. ниже впадения р. Туломы в Кольский залив. Минимальные IP-адреса: 109.124.97.0 - 109.124.97.3.`\n",
    "* Морфологический анализ (стемминг, лемматизация) - определение начальной формы слова или его псевдопрефикса, грамматических параметров. Подробнее описан ниже.\n",
    "* Синтаксический анализ - определение связей между словами (деревья зависимостей) или синтаксически связанных групп слов (деревья составляющих). Первые больше подходят для русского языка, вторые - для английского.\n",
    "\n",
    "<center><img src=\"https://upload.wikimedia.org/wikipedia/commons/0/0d/Wearetryingtounderstandthedifference_%282%29.jpg\">\n",
    "    <a href=\"https://en.wikipedia.org/wiki/Dependency_grammar\">Деревья зависимостей</a>\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/4/49/Competing_sentence_diagrams.png/750px-Competing_sentence_diagrams.png\">\n",
    "<a href=\"https://en.wikipedia.org/wiki/Constituent_(linguistics)\">Деревья составляющих</a></center>\n",
    "\n",
    "* Семантический анализ - определение смысла слова и работа с ним (`за'мок` vs `замо'к`, `удаление ребра связанного графа`; не путать с `он видел их семью своими глазами` где имеет место грамматическая неоднозначность `семья`-`семь`). Последнее время чаще используются дистрибутивные модели языка, о которых мы поговорим на отдельном занятии.\n",
    "\n",
    "Задачей морфологического анализа является определение начальной формы слова, его части речи и грамматических параметров. В некоторых случаях от слова требуется только начальная форма, в других - только начальная форма и часть речи.<br>\n",
    "Существует два больших подхода к морфологическому анализу: <b>стемминг</b> и <b>поиск по словарю</b>. Для проведения стемминга оставляется справочник всех окончаний для данного языка. Для пришедшего слова проверяется его окончание и по нему делается прогноз начальной формы и части речи.<br>\n",
    "Например, мы создаем справочник, в котором записываем все окончания прилагательных: <i>-ому, -ему, -ой, -ая, -ий, -ый, ...</i> Теперь все слова, которые имеют такое окончание будут считаться прилагаельными: <i>синий, циклический, красного, больному</i>. Заодно прилагательными будут считаться причастия (<i>делающий, строившему</i>) и местоимения (<i>мой, твой, твоему</i>). Также не понятно что делать со словами, имеющими пустое окончание. Отдельную проблему составляют такие слова, как <i>стекло, больной, вина</i>, которые могут разбираться несколькими вариантами (это явление называется <b>омонимией</b>). Помимо этого, стеммер может просто откусывать окончания, оставляя лишь псевдооснову.<br>\n",
    "Большинство проблем здесь решается, но точность работы бессловарных стеммеров находится на уровне 80%. Чтобы повысить точность испольуют морфологический анализ со словарем. Разработчики составляют словарь слов, встретившихся в текстах (<a href=\"http://opencorpora.org/dict.php\">здесь</a> можно найти пример такого словаря). Теперь каждое слово будет искаться в словаре и не предсказываться, а выдаваться точно. Для слов, отсутствующих в словаре, может применяться предсказание, пообное работе стеммера.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пусть к нам в руки попал файл с новостями с сайта http://lenta.ru/ . Нам любопытно посмотреть какую-то статистику по этому сайту и его новостям.\n",
    "\n",
    "Файл представляет собой просто текст с некоторым форматированием. Все новости отделены друг от друга пятью знаками равно, дальше идет дата новости, пять минусов, текст новости. \n",
    "\n",
    "Для начала загрузим новости в DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Морфологический анализ\n",
    "\n",
    "Есть несколько наиболее распространенных библиотеки для морфологического анализа текстов на Python: pymorphy2, pymystem и nltk. Рассмотрим работу с ними.\n",
    "\n",
    "Библиотеки pymorphy основана на словаре [OpenCorpora](opencorpora.org/) и позволяет проводить анализ отдельных слов, то есть предварительно необходимо провести графематический анализ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy2 # Морфологический анализатор."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на результаты анализа отдельного слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parse(word='стекло', tag=OpencorporaTag('NOUN,inan,neut sing,nomn'), normal_form='стекло', score=0.75, methods_stack=((<DictionaryAnalyzer>, 'стекло', 545, 0),)), Parse(word='стекло', tag=OpencorporaTag('NOUN,inan,neut sing,accs'), normal_form='стекло', score=0.1875, methods_stack=((<DictionaryAnalyzer>, 'стекло', 545, 3),)), Parse(word='стекло', tag=OpencorporaTag('VERB,perf,intr neut,sing,past,indc'), normal_form='стечь', score=0.0625, methods_stack=((<DictionaryAnalyzer>, 'стекло', 968, 3),))]\n"
     ]
    }
   ],
   "source": [
    "morph=pymorphy2.MorphAnalyzer() # Создает объект морфоанализатора и загружет словарь.\n",
    "wordform=morph.parse('стекло')  # Проведем анализ слова \"стекло\"...\n",
    "print(wordform)                 # ... и посмотрим на результат."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно из вывода, слово \"стекло\" может быть неодушевленным существительным среднего рода, единственного числа, именительного падежа `tag=OpencorporaTag('NOUN,inan,neut sing,nomn')`, аналогично, но в винительном падеже (`'NOUN,inan,neut sing,accs'`), и глаголом `'VERB,perf,intr neut,sing,past,indc'`. При этом в первой форме оно встречается в 75% случаев (<i>score=0.75</i>), во второй в 18,75% случаев (<i>score=0.1875</i>), а как глагол - лишь в 6,25% (<i>score=0.0625</i>). Самым простым видом борьбы с омонимией является выбор нулевого элемента из списка, возвращенного морфологическим анализом. Такой подход дает около 90% точности при выборе начальной формы и до 80% если мы обращаем внимание на грамматические параметры."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1875"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordform[1].score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpencorporaTag('NOUN,inan,neut sing,nomn')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordform[0].tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pymorphy2 умеет синтезировать нужные нам формы слова. Для этого необходимо получить объект типа `Parse` для нужного слова, а затем вызвать функцию `inflect`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parse(word='стёклам', tag=OpencorporaTag('NOUN,inan,neut plur,datv'), normal_form='стекло', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'стёклам', 545, 8),))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordform[0].inflect({'plur','datv'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Помимо этого можно получить вообще всю лексему."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parse(word='стекло', tag=OpencorporaTag('NOUN,inan,neut sing,nomn'), normal_form='стекло', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'стекло', 545, 0),)),\n",
       " Parse(word='стекла', tag=OpencorporaTag('NOUN,inan,neut sing,gent'), normal_form='стекло', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'стекла', 545, 1),)),\n",
       " Parse(word='стеклу', tag=OpencorporaTag('NOUN,inan,neut sing,datv'), normal_form='стекло', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'стеклу', 545, 2),)),\n",
       " Parse(word='стекло', tag=OpencorporaTag('NOUN,inan,neut sing,accs'), normal_form='стекло', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'стекло', 545, 3),)),\n",
       " Parse(word='стеклом', tag=OpencorporaTag('NOUN,inan,neut sing,ablt'), normal_form='стекло', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'стеклом', 545, 4),)),\n",
       " Parse(word='стекле', tag=OpencorporaTag('NOUN,inan,neut sing,loct'), normal_form='стекло', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'стекле', 545, 5),)),\n",
       " Parse(word='стёкла', tag=OpencorporaTag('NOUN,inan,neut plur,nomn'), normal_form='стекло', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'стёкла', 545, 6),)),\n",
       " Parse(word='стёкол', tag=OpencorporaTag('NOUN,inan,neut plur,gent'), normal_form='стекло', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'стёкол', 545, 7),)),\n",
       " Parse(word='стёклам', tag=OpencorporaTag('NOUN,inan,neut plur,datv'), normal_form='стекло', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'стёклам', 545, 8),)),\n",
       " Parse(word='стёкла', tag=OpencorporaTag('NOUN,inan,neut plur,accs'), normal_form='стекло', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'стёкла', 545, 9),)),\n",
       " Parse(word='стёклами', tag=OpencorporaTag('NOUN,inan,neut plur,ablt'), normal_form='стекло', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'стёклами', 545, 10),)),\n",
       " Parse(word='стёклах', tag=OpencorporaTag('NOUN,inan,neut plur,loct'), normal_form='стекло', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'стёклах', 545, 11),))]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordform[0].lexeme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание на `methods_stack=((<DictionaryAnalyzer>, 'лось', 121, 0)` и `methods_stack=((<FakeDictionary>, 'варкалось', 224, 9)`. Наличие строки <FakeDictionary> говорито том, что слово было предсказано. Причем в первом случае оно предсказано как форма слова _лось_, к которому добавлена неизвестная приставка. Во втором случае - это совершенно незнакомое слово."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А ещё pymorphy умеет предсказывать незнакомые слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parse(word='пырялись', tag=OpencorporaTag('VERB,impf,intr plur,past,indc'), normal_form='пыряться', score=1.0, methods_stack=((<FakeDictionary>, 'пырялись', 224, 10), (<KnownSuffixAnalyzer>, 'ялись')))]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordform=morph.parse('пырялись') \n",
    "wordform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вместо Pymorphy можно использовать PyMystem. Его плюсом является тот факт, что он сам проводит графематический анализ и снимает омонимию. \n",
    "\n",
    "Функция `lemmatize` делит текст на слова и знаки препинания, а затем возвращает для них только начальную форму.\n",
    "\n",
    "Функция `analyze` возвращает не только начальную форму, но и всю информацию о слове, как это делал перед этим Pymorphy. \n",
    "\n",
    "Как видно из примера, делает он это не всегда корректно, но нам не придется думать о том, какое вариант разбора следует взять."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymystem3 # Еще один морфологический анализатор. При первом запуске грузит словари из Сети."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['этот', ' ', 'тип', ' ', 'становиться', ' ', 'есть', ' ', 'в', ' ', 'цех', '.', '\\n']\n",
      "[{'analysis': [{'lex': 'этот', 'wt': 1, 'gr': 'APRO=(им,мн|вин,мн,неод)'}], 'text': 'эти'}, {'text': ' '}, {'analysis': [{'lex': 'тип', 'wt': 0.8700298667, 'gr': 'S,муж,неод=(вин,мн|им,мн)'}], 'text': 'типы'}, {'text': ' '}, {'analysis': [{'lex': 'становиться', 'wt': 0.9821285009, 'gr': 'V,нп=прош,мн,изъяв,сов'}], 'text': 'стали'}, {'text': ' '}, {'analysis': [{'lex': 'есть', 'wt': 0.04922361672, 'gr': 'V,несов,пе=инф'}], 'text': 'есть'}, {'text': ' '}, {'analysis': [{'lex': 'в', 'wt': 0.9999917746, 'gr': 'PR='}], 'text': 'в'}, {'text': ' '}, {'analysis': [{'lex': 'цех', 'wt': 1, 'gr': 'S,муж,неод=(дат,ед|местн,ед)'}], 'text': 'цеху'}, {'text': '.'}, {'text': '\\n'}]\n"
     ]
    }
   ],
   "source": [
    "mystem=pymystem3.Mystem()\n",
    "print(mystem.lemmatize('эти типы стали есть в цеху.'))\n",
    "print(mystem.analyze('эти типы стали есть в цеху.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результаты работы можно представить в таком виде.\n",
    "\n",
    "```\n",
    "['этот', \n",
    " ' ',\n",
    " 'тип',\n",
    " ' ',\n",
    " 'становиться',\n",
    " ' ',\n",
    " 'есть',\n",
    " ' ',\n",
    " 'в',\n",
    " ' ',\n",
    " 'цех',\n",
    " '.',\n",
    " '\\n']```\n",
    " \n",
    "```\n",
    "[\n",
    "  {'analysis': \n",
    "    [{'lex': 'этот', 'wt': 1, 'gr': 'APRO=(им,мн|вин,мн,неод)'}], \n",
    "   'text': 'эти'\n",
    "  }, \n",
    "  {\n",
    "    'text': ' '\n",
    "  }, \n",
    "  {'analysis': \n",
    "    [{'lex': 'тип', 'wt': 0.8700298667, 'gr': 'S,муж,неод=(вин,мн|им,мн)'}], \n",
    "   'text': 'типы'}, \n",
    "  {'text': ' '}, \n",
    "  {'analysis': \n",
    "    [{'lex': 'становиться', 'wt': 0.9821285009, 'gr': 'V,нп=прош,мн,изъяв,сов'}], \n",
    "   'text': 'стали'}, \n",
    "  {'text': ' '}, \n",
    "  {'analysis': \n",
    "    [{'lex': 'есть', 'wt': 0.04922361672, 'gr': 'V,несов,пе=инф'}], \n",
    "   'text': 'есть'}, \n",
    "  {'text': ' '}, \n",
    "  {'analysis': \n",
    "    [{'lex': 'в', 'wt': 0.9999917746, 'gr': 'PR='}], \n",
    "   'text': 'в'}, \n",
    "  {'text': ' '}, \n",
    "  {'analysis': \n",
    "    [{'lex': 'цех', 'wt': 1, 'gr': 'S,муж,неод=(дат,ед|местн,ед)'}], \n",
    "   'text': 'цеху'}, \n",
    "  {'text': '.'}, \n",
    "  {'text': '\\n'}\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То есть результатом является список токенов (в том числе и пробельных или знаков препинания), для части из которых имеется результат анализа, который хранится в словаре с ключём `analysis`. Анализ хранит в списке один или несколько вариантов разбора, у каждого из которых есть лемма `lex`, набор грамматических параметров `gr` и некоторый вес `wt`, который показывает степень уверенности системы в правильности ответа."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APRO\n"
     ]
    }
   ],
   "source": [
    "my_res=mystem.analyze('эти типы стали есть в цеху.')\n",
    "if 'analysis' in my_res[0].keys(): # Проверяем, что это не разделитель.\n",
    "    print(my_res[0]['analysis'][0]['gr'].split(\"=\")[0]) # Берем из него анализ, из того грамматическсие параметы, \n",
    "                                                        # а из них выделяем часть речи."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Еще одна библиотека - NLTK. По сравнению с двумя предыдущими библиотеками она обладает более широкой функциональностью и изначально писалась для работы с разными языками."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk # Иностранный морфологический анализатор."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перед началом использования необходимо загрузить необходимые библиотеки или корпуса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Downloader\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> l\n",
      "\n",
      "Packages:\n",
      "  [ ] abc................. Australian Broadcasting Commission 2006\n",
      "  [ ] alpino.............. Alpino Dutch Treebank\n",
      "  [ ] averaged_perceptron_tagger Averaged Perceptron Tagger\n",
      "  [*] averaged_perceptron_tagger_ru Averaged Perceptron Tagger (Russian)\n",
      "  [ ] basque_grammars..... Grammars for Basque\n",
      "  [ ] biocreative_ppi..... BioCreAtIvE (Critical Assessment of Information\n",
      "                           Extraction Systems in Biology)\n",
      "  [ ] bllip_wsj_no_aux.... BLLIP Parser: WSJ Model\n",
      "  [ ] book_grammars....... Grammars from NLTK Book\n",
      "  [ ] brown............... Brown Corpus\n",
      "  [ ] brown_tei........... Brown Corpus (TEI XML Version)\n",
      "  [ ] cess_cat............ CESS-CAT Treebank\n",
      "  [ ] cess_esp............ CESS-ESP Treebank\n",
      "  [ ] chat80.............. Chat-80 Data Files\n",
      "  [ ] city_database....... City Database\n",
      "  [ ] cmudict............. The Carnegie Mellon Pronouncing Dictionary (0.6)\n",
      "  [ ] comparative_sentences Comparative Sentence Dataset\n",
      "  [ ] comtrans............ ComTrans Corpus Sample\n",
      "  [ ] conll2000........... CONLL 2000 Chunking Corpus\n",
      "  [ ] conll2002........... CONLL 2002 Named Entity Recognition Corpus\n",
      "Hit Enter to continue: \n",
      "  [ ] conll2007........... Dependency Treebanks from CoNLL 2007 (Catalan\n",
      "                           and Basque Subset)\n",
      "  [ ] crubadan............ Crubadan Corpus\n",
      "  [ ] dependency_treebank. Dependency Parsed Treebank\n",
      "  [ ] dolch............... Dolch Word List\n",
      "  [ ] europarl_raw........ Sample European Parliament Proceedings Parallel\n",
      "                           Corpus\n",
      "  [ ] floresta............ Portuguese Treebank\n",
      "  [ ] framenet_v15........ FrameNet 1.5\n",
      "  [ ] framenet_v17........ FrameNet 1.7\n",
      "  [ ] gazetteers.......... Gazeteer Lists\n",
      "  [ ] genesis............. Genesis Corpus\n",
      "  [ ] gutenberg........... Project Gutenberg Selections\n",
      "  [ ] ieer................ NIST IE-ER DATA SAMPLE\n",
      "  [ ] inaugural........... C-Span Inaugural Address Corpus\n",
      "  [ ] indian.............. Indian Language POS-Tagged Corpus\n",
      "  [ ] jeita............... JEITA Public Morphologically Tagged Corpus (in\n",
      "                           ChaSen format)\n",
      "  [ ] kimmo............... PC-KIMMO Data Files\n",
      "  [ ] knbc................ KNB Corpus (Annotated blog corpus)\n",
      "  [ ] large_grammars...... Large context-free and feature-based grammars\n",
      "                           for parser comparison\n",
      "Hit Enter to continue: \n",
      "  [ ] lin_thesaurus....... Lin's Dependency Thesaurus\n",
      "  [ ] mac_morpho.......... MAC-MORPHO: Brazilian Portuguese news text with\n",
      "                           part-of-speech tags\n",
      "  [ ] machado............. Machado de Assis -- Obra Completa\n",
      "  [ ] masc_tagged......... MASC Tagged Corpus\n",
      "  [ ] maxent_ne_chunker... ACE Named Entity Chunker (Maximum entropy)\n",
      "  [ ] maxent_treebank_pos_tagger Treebank Part of Speech Tagger (Maximum entropy)\n",
      "  [ ] moses_sample........ Moses Sample Models\n",
      "  [ ] movie_reviews....... Sentiment Polarity Dataset Version 2.0\n",
      "  [ ] mte_teip5........... MULTEXT-East 1984 annotated corpus 4.0\n",
      "  [ ] mwa_ppdb............ The monolingual word aligner (Sultan et al.\n",
      "                           2015) subset of the Paraphrase Database.\n",
      "  [ ] names............... Names Corpus, Version 1.3 (1994-03-29)\n",
      "  [ ] nombank.1.0......... NomBank Corpus 1.0\n",
      "  [ ] nonbreaking_prefixes Non-Breaking Prefixes (Moses Decoder)\n",
      "  [ ] nps_chat............ NPS Chat\n",
      "  [ ] omw................. Open Multilingual Wordnet\n",
      "  [ ] opinion_lexicon..... Opinion Lexicon\n",
      "  [ ] panlex_swadesh...... PanLex Swadesh Corpora\n",
      "  [ ] paradigms........... Paradigm Corpus\n",
      "  [ ] pe08................ Cross-Framework and Cross-Domain Parser\n",
      "                           Evaluation Shared Task\n",
      "Hit Enter to continue: \n",
      "  [ ] perluniprops........ perluniprops: Index of Unicode Version 7.0.0\n",
      "                           character properties in Perl\n",
      "  [ ] pil................. The Patient Information Leaflet (PIL) Corpus\n",
      "  [ ] pl196x.............. Polish language of the XX century sixties\n",
      "  [ ] porter_test......... Porter Stemmer Test Files\n",
      "  [ ] ppattach............ Prepositional Phrase Attachment Corpus\n",
      "  [ ] problem_reports..... Problem Report Corpus\n",
      "  [ ] product_reviews_1... Product Reviews (5 Products)\n",
      "  [ ] product_reviews_2... Product Reviews (9 Products)\n",
      "  [ ] propbank............ Proposition Bank Corpus 1.0\n",
      "  [ ] pros_cons........... Pros and Cons\n",
      "  [ ] ptb................. Penn Treebank\n",
      "  [*] punkt............... Punkt Tokenizer Models\n",
      "  [ ] qc.................. Experimental Data for Question Classification\n",
      "  [ ] reuters............. The Reuters-21578 benchmark corpus, ApteMod\n",
      "                           version\n",
      "  [ ] rslp................ RSLP Stemmer (Removedor de Sufixos da Lingua\n",
      "                           Portuguesa)\n",
      "  [ ] rte................. PASCAL RTE Challenges 1, 2, and 3\n",
      "  [ ] sample_grammars..... Sample Grammars\n",
      "  [ ] semcor.............. SemCor 3.0\n",
      "Hit Enter to continue: \n",
      "  [ ] senseval............ SENSEVAL 2 Corpus: Sense Tagged Text\n",
      "  [ ] sentence_polarity... Sentence Polarity Dataset v1.0\n",
      "  [ ] sentiwordnet........ SentiWordNet\n",
      "  [ ] shakespeare......... Shakespeare XML Corpus Sample\n",
      "  [ ] sinica_treebank..... Sinica Treebank Corpus Sample\n",
      "  [ ] smultron............ SMULTRON Corpus Sample\n",
      "  [ ] snowball_data....... Snowball Data\n",
      "  [ ] spanish_grammars.... Grammars for Spanish\n",
      "  [ ] state_union......... C-Span State of the Union Address Corpus\n",
      "  [ ] stopwords........... Stopwords Corpus\n",
      "  [ ] subjectivity........ Subjectivity Dataset v1.0\n",
      "  [ ] swadesh............. Swadesh Wordlists\n",
      "  [ ] switchboard......... Switchboard Corpus Sample\n",
      "  [ ] tagsets............. Help on Tagsets\n",
      "  [ ] timit............... TIMIT Corpus Sample\n",
      "  [ ] toolbox............. Toolbox Sample Files\n",
      "  [ ] treebank............ Penn Treebank Sample\n",
      "  [ ] twitter_samples..... Twitter Samples\n",
      "  [ ] udhr2............... Universal Declaration of Human Rights Corpus\n",
      "                           (Unicode Version)\n",
      "  [ ] udhr................ Universal Declaration of Human Rights Corpus\n",
      "Hit Enter to continue: \n",
      "  [ ] unicode_samples..... Unicode Samples\n",
      "  [ ] universal_tagset.... Mappings to the Universal Part-of-Speech Tagset\n",
      "  [ ] universal_treebanks_v20 Universal Treebanks Version 2.0\n",
      "  [ ] vader_lexicon....... VADER Sentiment Lexicon\n",
      "  [ ] verbnet3............ VerbNet Lexicon, Version 3.3\n",
      "  [ ] verbnet............. VerbNet Lexicon, Version 2.1\n",
      "  [ ] webtext............. Web Text Corpus\n",
      "  [ ] wmt15_eval.......... Evaluation data from WMT15\n",
      "  [ ] word2vec_sample..... Word2Vec Sample\n",
      "  [ ] wordnet............. WordNet\n",
      "  [ ] wordnet_ic.......... WordNet-InfoContent\n",
      "  [ ] words............... Word Lists\n",
      "  [ ] ycoe................ York-Toronto-Helsinki Parsed Corpus of Old\n",
      "                           English Prose\n",
      "\n",
      "Collections:\n",
      "  [ ] all-corpora......... All the corpora\n",
      "  [P] all-nltk............ All packages available on nltk_data gh-pages\n",
      "                           branch\n",
      "  [P] all................. All packages\n",
      "  [P] book................ Everything used in the NLTK Book\n",
      "  [P] popular............. Popular packages\n",
      "Hit Enter to continue: \n",
      "  [ ] tests............... Packages for running tests\n",
      "  [ ] third-party......... Third-party data packages\n",
      "\n",
      "([*] marks installed packages; [P] marks partially installed collections)\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> d\n",
      "\n",
      "Download which package (l=list; x=cancel)?\n",
      "  Identifier> averaged_perceptron_tagger\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    Downloading package averaged_perceptron_tagger to\n",
      "        /home/edward/nltk_data...\n",
      "      Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> q\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download() # По дороге будут появляться поле ввода. Грузит всё из Сети."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно сразу скачать нужный пакет, если вы знаете как он назыввается."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]     /home/edward/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_ru is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to /home/edward/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(['averaged_perceptron_tagger_ru', 'stopwords'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция `word_tokenize` возвращает начальные формы слов. \n",
    "\n",
    "Функция `pos_tag` возвращает список начальных форм и их частей речи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Эти', 'типы', 'стали', 'есть', 'в', 'цеху'],\n",
       " [('Эти', 'типы'),\n",
       "  ('типы', 'стали'),\n",
       "  ('стали', 'есть'),\n",
       "  ('есть', 'в'),\n",
       "  ('в', 'цеху')])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize('Эти типы стали есть в цеху') # Токенизация.\n",
    "bi_tokens = list(nltk.bigrams(tokens))\n",
    "tokens, bi_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('Эти', 'JJ'),\n",
       "  ('типы', 'NNP'),\n",
       "  ('стали', 'NNP'),\n",
       "  ('есть', 'NNP'),\n",
       "  ('в', 'NNP'),\n",
       "  ('цеху', 'NN')],\n",
       " [(('Эти', 'JJ'), ('типы', 'NNP')),\n",
       "  (('типы', 'NNP'), ('стали', 'NNP')),\n",
       "  (('стали', 'NNP'), ('есть', 'NNP')),\n",
       "  (('есть', 'NNP'), ('в', 'NNP')),\n",
       "  (('в', 'NNP'), ('цеху', 'NN'))])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos = nltk.pos_tag(tokens) # Частеречная разметка.\n",
    "bi_pos = list(nltk.bigrams(pos))\n",
    "pos, bi_pos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У NLTK заведен список стоп-слов, которые лучше фильтровать при анализе текстов. Но их не очень много. Зато самые мешающиеся."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "всего русских стоп-слов 151\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Эти', 'типы', 'стали', 'цеху']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Оставим только те слова, которых нет в списке стоп-слов.\n",
    "filtered_words = [token for token in tokens if token not in nltk.corpus.stopwords.words('russian')]\n",
    "print('всего русских стоп-слов', len(nltk.corpus.stopwords.words('russian')))\n",
    "filtered_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ту же самую задачу в других библиотеках можно решить при помощи фильтра частей речи. Можно считать, что значимыми являются лишь существительные, прилагательные, глаголы, причастия и деепричастия. Ниже приведены названия частей речи для разных библиотек.\n",
    "<table>\n",
    "<tr><th>Часть речи</th><th>Pymorphy</th><th>Mystem</th><th>NLTK</th></tr>\n",
    "<tr><td>Существительное</td><td>NOUN</td><td>S</td><td>NN</td></tr>\n",
    "<tr><td>Прилагательное</td><td>ADJF, ADJS</td><td>A</td><td>NNP</td></tr>\n",
    "<tr><td>Глагол</td><td>VERB</td><td>V</td><td>JJ</td></tr>\n",
    "<tr><td>Причастие</td><td>PRTF, PRTS</td><td>V</td><td>NNP</td></tr>\n",
    "<tr><td>Деепричастие</td><td>GRND</td><td>V</td><td>NNP</td></tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмем новость с сайта Лента.ру и приведем все слова её текста к начальным формам при помощи разных библиотек. Прибавим при этом к словам части речи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = requests.get('https://lenta.ru/news/2021/02/27/apple_effect/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обнаружен неожиданный эффект от употребления яблок\n",
      "15:35, 27 февраля 2021\n",
      "Соня Кошечкина\n",
      "Содержащиеся в этих плодах фитонутриенты способствуют образованию новых нейронов\n",
      "\n",
      " Ученые из Университета Квинсленда и Немецкого центра нейродегенеративных заболеваний обнаружили неожиданный эффект от употребления яблок. Результаты исследования появились в научном журнале Stem Cell Reports.\n",
      "Опыты проводились на мышах. Специалисты культивировали стволовые клетки мозга взрослых мышей и добавляли в них содержащиеся в яблоках фитонутриенты. Исследование показало, что высокая концентрация фитонутриентов способствует образованию новых нейронов.\n",
      "По словам ученых, определенные фитонутриенты положительно влияют на работу органов, в том числе мозга. Выяснилось, что они оказывают на организм тот же эффект, что и физическая активность, которая также стимулирует нейрогенез.\n",
      "Ранее ученые из Технологического университета австрийского Граца выяснили, что большинство людей неправильно едят яблоки. Исследователи утверждают, что до 90 процентов полезных веществ сосредоточены в сердцевине этого фрукта, и поэтому яблоко желательно съедать вместе с огрызком.\n"
     ]
    }
   ],
   "source": [
    "souped = bs(page.text)\n",
    "\n",
    "title = souped(\"h1\")[0].get_text()\n",
    "when = souped.find_all(\"div\", attrs={'class': 'b-topic__info'})[0](\"time\")[0].get_text().strip()\n",
    "author = souped.find_all(\"span\", attrs={'itemprop': 'name'})[0].get_text()\n",
    "description = souped.find_all(\"meta\", attrs={'itemprop': 'description'})[0][\"content\"]\n",
    "text = '\\n'.join([p.get_text() for p in souped.find_all(\"div\", attrs={'itemprop': 'articleBody'})[0](\"p\")])\n",
    "\n",
    "print(title)\n",
    "print(when)\n",
    "print(author)\n",
    "print(description)\n",
    "print('\\n', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['учёный_NOUN',\n",
       " 'из_PREP',\n",
       " 'университет_NOUN',\n",
       " 'квинсленд_NOUN',\n",
       " 'и_CONJ',\n",
       " 'немецкий_ADJF',\n",
       " 'центр_NOUN',\n",
       " 'нейродегенеративный_ADJF',\n",
       " 'заболевание_NOUN',\n",
       " 'обнаружить_VERB',\n",
       " 'неожиданный_ADJF',\n",
       " 'эффект_NOUN',\n",
       " 'от_PREP',\n",
       " 'употребление_NOUN',\n",
       " 'яблоко_NOUN',\n",
       " 'результат_NOUN',\n",
       " 'исследование_NOUN',\n",
       " 'появиться_VERB',\n",
       " 'в_PREP',\n",
       " 'научный_ADJF',\n",
       " 'журнал_NOUN',\n",
       " 'stem_None',\n",
       " 'cell_None',\n",
       " 'reports_None',\n",
       " 'опыт_NOUN',\n",
       " 'проводиться_VERB',\n",
       " 'на_PREP',\n",
       " 'мышь_NOUN',\n",
       " 'специалист_NOUN',\n",
       " 'культивировать_VERB',\n",
       " 'стволовой_ADJF',\n",
       " 'клетка_NOUN',\n",
       " 'мозг_NOUN',\n",
       " 'взрослый_NOUN',\n",
       " 'мышей_NOUN',\n",
       " 'и_CONJ',\n",
       " 'добавлять_VERB',\n",
       " 'в_PREP',\n",
       " 'они_NPRO',\n",
       " 'содержаться_PRTF',\n",
       " 'в_PREP',\n",
       " 'яблоко_NOUN',\n",
       " 'фитонутриент_NOUN',\n",
       " 'исследование_NOUN',\n",
       " 'показать_VERB',\n",
       " 'что_CONJ',\n",
       " 'высокий_ADJF',\n",
       " 'концентрация_NOUN',\n",
       " 'фитонутриент_NOUN',\n",
       " 'способствовать_VERB',\n",
       " 'образование_NOUN',\n",
       " 'новый_ADJF',\n",
       " 'нейрон_NOUN',\n",
       " 'по_PREP',\n",
       " 'слово_NOUN',\n",
       " 'учёный_ADJF',\n",
       " 'определённый_ADJF',\n",
       " 'фитонутриент_NOUN',\n",
       " 'положительно_ADVB',\n",
       " 'влиять_VERB',\n",
       " 'на_PREP',\n",
       " 'работа_NOUN',\n",
       " 'орган_NOUN',\n",
       " 'в_PREP',\n",
       " 'тот_ADJF',\n",
       " 'число_NOUN',\n",
       " 'мозг_NOUN',\n",
       " 'выясниться_VERB',\n",
       " 'что_CONJ',\n",
       " 'они_NPRO',\n",
       " 'оказывать_VERB',\n",
       " 'на_PREP',\n",
       " 'организм_NOUN',\n",
       " 'тот_ADJF',\n",
       " 'же_PRCL',\n",
       " 'эффект_NOUN',\n",
       " 'что_CONJ',\n",
       " 'и_CONJ',\n",
       " 'физический_ADJF',\n",
       " 'активность_NOUN',\n",
       " 'который_ADJF',\n",
       " 'также_CONJ',\n",
       " 'стимулировать_VERB',\n",
       " 'нейрогенез_NOUN',\n",
       " 'ранее_ADVB',\n",
       " 'учёный_NOUN',\n",
       " 'из_PREP',\n",
       " 'технологический_ADJF',\n",
       " 'университет_NOUN',\n",
       " 'австрийский_ADJF',\n",
       " 'грац_NOUN',\n",
       " 'выяснить_VERB',\n",
       " 'что_CONJ',\n",
       " 'большинство_NOUN',\n",
       " 'человек_NOUN',\n",
       " 'неправильно_ADVB',\n",
       " 'есть_VERB',\n",
       " 'яблоко_NOUN',\n",
       " 'исследователь_NOUN',\n",
       " 'утверждать_VERB',\n",
       " 'что_CONJ',\n",
       " 'до_PREP',\n",
       " 'процент_NOUN',\n",
       " 'полезный_ADJF',\n",
       " 'вещество_NOUN',\n",
       " 'сосредоточить_PRTS',\n",
       " 'в_PREP',\n",
       " 'сердцевина_NOUN',\n",
       " 'это_NPRO',\n",
       " 'фрукт_NOUN',\n",
       " 'и_CONJ',\n",
       " 'поэтому_ADVB',\n",
       " 'яблоко_NOUN',\n",
       " 'желательно_ADVB',\n",
       " 'съедать_INFN',\n",
       " 'вместе_ADVB',\n",
       " 'с_PREP',\n",
       " 'огрызок_NOUN']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pymorphy\n",
    "def normalizePymorphy(text):\n",
    "    tokens = re.findall('[A-Za-zА-Яа-яЁё]+\\-[A-Za-zА-Яа-яЁё]+|[A-Za-zА-Яа-яЁё]+', text)\n",
    "    words = []\n",
    "    for t in tokens:\n",
    "        pv = morph.parse(t)\n",
    "        words.append(pv[0].normal_form + '_' + str(pv[0].tag.POS)) # Берем наиболее вероятную форму.\n",
    "    return words    \n",
    "        \n",
    "# Обратите внимание, что про иностранные слова словарь ничего не знает.\n",
    "normalizePymorphy(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ученый_S',\n",
       " 'из_P',\n",
       " 'университет_S',\n",
       " 'квинсленд_S',\n",
       " 'и_C',\n",
       " 'немецкий_A',\n",
       " 'центр_S',\n",
       " 'нейродегенеративный_A',\n",
       " 'заболевание_S',\n",
       " 'обнаруживать_V',\n",
       " 'неожиданный_A',\n",
       " 'эффект_S',\n",
       " 'от_P',\n",
       " 'употребление_S',\n",
       " 'яблоко_S',\n",
       " 'результат_S',\n",
       " 'исследование_S',\n",
       " 'появляться_V',\n",
       " 'в_P',\n",
       " 'научный_A',\n",
       " 'журнал_S',\n",
       " 'Stem_U',\n",
       " 'Cell_U',\n",
       " 'Reports_U',\n",
       " 'опыт_S',\n",
       " 'проводиться_V',\n",
       " 'на_P',\n",
       " 'мышь_S',\n",
       " 'специалист_S',\n",
       " 'культивировать_V',\n",
       " 'стволовой_A',\n",
       " 'клетка_S',\n",
       " 'мозг_S',\n",
       " 'взрослый_S',\n",
       " 'мышь_S',\n",
       " 'и_C',\n",
       " 'добавлять_V',\n",
       " 'в_P',\n",
       " 'они_S',\n",
       " 'содержаться_V',\n",
       " 'в_P',\n",
       " 'яблоко_S',\n",
       " 'фитонутриент_S',\n",
       " 'исследование_S',\n",
       " 'показывать_V',\n",
       " 'что_C',\n",
       " 'высокий_A',\n",
       " 'концентрация_S',\n",
       " 'фитонутриент_S',\n",
       " 'способствовать_V',\n",
       " 'образование_S',\n",
       " 'новый_A',\n",
       " 'нейрон_S',\n",
       " 'по_P',\n",
       " 'слово_S',\n",
       " 'ученый_S',\n",
       " 'определенный_A',\n",
       " 'фитонутриент_S',\n",
       " 'положительно_A',\n",
       " 'влиять_V',\n",
       " 'на_P',\n",
       " 'работа_S',\n",
       " 'орган_S',\n",
       " 'в_P',\n",
       " 'тот_A',\n",
       " 'число_S',\n",
       " 'мозг_S',\n",
       " 'выясняться_V',\n",
       " 'что_C',\n",
       " 'они_S',\n",
       " 'оказывать_V',\n",
       " 'на_P',\n",
       " 'организм_S',\n",
       " 'тот_A',\n",
       " 'же_P',\n",
       " 'эффект_S',\n",
       " 'что_C',\n",
       " 'и_C',\n",
       " 'физический_A',\n",
       " 'активность_S',\n",
       " 'который_A',\n",
       " 'также_A',\n",
       " 'стимулировать_V',\n",
       " 'нейрогенез_S',\n",
       " 'ранее_A',\n",
       " 'ученый_S',\n",
       " 'из_P',\n",
       " 'технологический_A',\n",
       " 'университет_S',\n",
       " 'австрийский_A',\n",
       " 'грац_S',\n",
       " 'выяснять_V',\n",
       " 'что_C',\n",
       " 'большинство_S',\n",
       " 'человек_S',\n",
       " 'неправильно_A',\n",
       " 'есть_V',\n",
       " 'яблоко_S',\n",
       " 'исследователь_S',\n",
       " 'утверждать_V',\n",
       " 'что_C',\n",
       " 'до_P',\n",
       " 'процент_S',\n",
       " 'полезный_A',\n",
       " 'вещество_S',\n",
       " 'сосредотачивать_V',\n",
       " 'в_P',\n",
       " 'сердцевина_S',\n",
       " 'этот_A',\n",
       " 'фрукт_S',\n",
       " 'и_C',\n",
       " 'поэтому_A',\n",
       " 'яблоко_S',\n",
       " 'желательно_A',\n",
       " 'съедать_V',\n",
       " 'вместе_A',\n",
       " 'с_P',\n",
       " 'огрызок_S']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PyMystem\n",
    "def normalizePymystem(text):\n",
    "    tokens = mystem.analyze(text)\n",
    "    words = []\n",
    "    for t in tokens:\n",
    "        if 'analysis' in t.keys():\n",
    "            if t['analysis'] != []:\n",
    "                words.append(t['analysis'][0]['lex']+'_'+t['analysis'][0]['gr'][0])\n",
    "            else:\n",
    "                words.append(t['text']+'_'+'U')\n",
    "    return words    \n",
    "        \n",
    "# Не все считают, что причастие всегда выступает в роли глагола, но иногда так значительно проще.\n",
    "normalizePymystem(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ученые_JJ',\n",
       " 'из_NNP',\n",
       " 'Университета_NNP',\n",
       " 'Квинсленда_NNP',\n",
       " 'и_NNP',\n",
       " 'Немецкого_NNP',\n",
       " 'центра_NNP',\n",
       " 'нейродегенеративных_NNP',\n",
       " 'заболеваний_NNP',\n",
       " 'обнаружили_NNP',\n",
       " 'неожиданный_NNP',\n",
       " 'эффект_NNP',\n",
       " 'от_NNP',\n",
       " 'употребления_NNP',\n",
       " 'яблок_NNP',\n",
       " 'Результаты_VB',\n",
       " 'исследования_JJ',\n",
       " 'появились_NNP',\n",
       " 'в_NNP',\n",
       " 'научном_NNP',\n",
       " 'журнале_NNP',\n",
       " 'Stem_NNP',\n",
       " 'Cell_NNP',\n",
       " 'Reports_NNP',\n",
       " 'Опыты_VB',\n",
       " 'проводились_JJ',\n",
       " 'на_NNP',\n",
       " 'мышах_NNP',\n",
       " 'Специалисты_VB',\n",
       " 'культивировали_JJ',\n",
       " 'стволовые_NNP',\n",
       " 'клетки_NNP',\n",
       " 'мозга_NNP',\n",
       " 'взрослых_NNP',\n",
       " 'мышей_NNP',\n",
       " 'и_NNP',\n",
       " 'добавляли_NNP',\n",
       " 'в_NNP',\n",
       " 'них_NNP',\n",
       " 'содержащиеся_NNP',\n",
       " 'в_NNP',\n",
       " 'яблоках_NNP',\n",
       " 'фитонутриенты_NNP',\n",
       " 'Исследование_NN',\n",
       " 'показало_NN',\n",
       " 'что_NNP',\n",
       " 'высокая_NNP',\n",
       " 'концентрация_NNP',\n",
       " 'фитонутриентов_NNP',\n",
       " 'способствует_NNP',\n",
       " 'образованию_NNP',\n",
       " 'новых_NNP',\n",
       " 'нейронов_NNP',\n",
       " 'По_VB',\n",
       " 'словам_JJ',\n",
       " 'ученых_NNP',\n",
       " 'определенные_NNP',\n",
       " 'фитонутриенты_NNP',\n",
       " 'положительно_NNP',\n",
       " 'влияют_NNP',\n",
       " 'на_NNP',\n",
       " 'работу_NNP',\n",
       " 'органов_NNP',\n",
       " 'в_NNP',\n",
       " 'том_NNP',\n",
       " 'числе_NNP',\n",
       " 'мозга_NNP',\n",
       " 'Выяснилось_NN',\n",
       " 'что_NNP',\n",
       " 'они_NNP',\n",
       " 'оказывают_NNP',\n",
       " 'на_NNP',\n",
       " 'организм_NNP',\n",
       " 'тот_NNP',\n",
       " 'же_NNP',\n",
       " 'эффект_NNP',\n",
       " 'что_NNP',\n",
       " 'и_NNP',\n",
       " 'физическая_NNP',\n",
       " 'активность_NNP',\n",
       " 'которая_NNP',\n",
       " 'также_NNP',\n",
       " 'стимулирует_NNP',\n",
       " 'нейрогенез_NNP',\n",
       " 'Ранее_VB',\n",
       " 'ученые_JJ',\n",
       " 'из_NNP',\n",
       " 'Технологического_NNP',\n",
       " 'университета_NNP',\n",
       " 'австрийского_NNP',\n",
       " 'Граца_NNP',\n",
       " 'выяснили_NNP',\n",
       " 'что_NNP',\n",
       " 'большинство_NNP',\n",
       " 'людей_NNP',\n",
       " 'неправильно_NNP',\n",
       " 'едят_NNP',\n",
       " 'яблоки_NNP',\n",
       " 'Исследователи_NN',\n",
       " 'утверждают_NN',\n",
       " 'что_NNP',\n",
       " 'до_VBZ',\n",
       " '90_CD',\n",
       " 'процентов_NN',\n",
       " 'полезных_NNP',\n",
       " 'веществ_NNP',\n",
       " 'сосредоточены_NNP',\n",
       " 'в_NNP',\n",
       " 'сердцевине_NNP',\n",
       " 'этого_NNP',\n",
       " 'фрукта_NNP',\n",
       " 'и_NNP',\n",
       " 'поэтому_NNP',\n",
       " 'яблоко_NNP',\n",
       " 'желательно_NNP',\n",
       " 'съедать_NNP',\n",
       " 'вместе_NNP',\n",
       " 'с_NNP',\n",
       " 'огрызком_NNP']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NLTK\n",
    "def normalizeNLTK(text):\n",
    "    tokens = nltk.pos_tag(nltk.word_tokenize(text))\n",
    "    words = []\n",
    "    for t in tokens:\n",
    "        if t[0] != t[1]:\n",
    "            words.append(t[0]+'_'+t[1])\n",
    "    return words    \n",
    "        \n",
    "# А вот здесь с частеречной разметкой всё плохо, а параметров нет вовсе.\n",
    "normalizeNLTK(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter # Не считать же частоты самим."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'в_PREP': 5, 'что_CONJ': 5, 'и_CONJ': 4, 'яблоко_NOUN': 4, 'на_PREP': 3, 'фитонутриент_NOUN': 3, 'учёный_NOUN': 2, 'из_PREP': 2, 'университет_NOUN': 2, 'эффект_NOUN': 2, 'исследование_NOUN': 2, 'мозг_NOUN': 2, 'они_NPRO': 2, 'тот_ADJF': 2, 'квинсленд_NOUN': 1, 'немецкий_ADJF': 1, 'центр_NOUN': 1, 'нейродегенеративный_ADJF': 1, 'заболевание_NOUN': 1, 'обнаружить_VERB': 1, 'неожиданный_ADJF': 1, 'от_PREP': 1, 'употребление_NOUN': 1, 'результат_NOUN': 1, 'появиться_VERB': 1, 'научный_ADJF': 1, 'журнал_NOUN': 1, 'stem_None': 1, 'cell_None': 1, 'reports_None': 1, 'опыт_NOUN': 1, 'проводиться_VERB': 1, 'мышь_NOUN': 1, 'специалист_NOUN': 1, 'культивировать_VERB': 1, 'стволовой_ADJF': 1, 'клетка_NOUN': 1, 'взрослый_NOUN': 1, 'мышей_NOUN': 1, 'добавлять_VERB': 1, 'содержаться_PRTF': 1, 'показать_VERB': 1, 'высокий_ADJF': 1, 'концентрация_NOUN': 1, 'способствовать_VERB': 1, 'образование_NOUN': 1, 'новый_ADJF': 1, 'нейрон_NOUN': 1, 'по_PREP': 1, 'слово_NOUN': 1, 'учёный_ADJF': 1, 'определённый_ADJF': 1, 'положительно_ADVB': 1, 'влиять_VERB': 1, 'работа_NOUN': 1, 'орган_NOUN': 1, 'число_NOUN': 1, 'выясниться_VERB': 1, 'оказывать_VERB': 1, 'организм_NOUN': 1, 'же_PRCL': 1, 'физический_ADJF': 1, 'активность_NOUN': 1, 'который_ADJF': 1, 'также_CONJ': 1, 'стимулировать_VERB': 1, 'нейрогенез_NOUN': 1, 'ранее_ADVB': 1, 'технологический_ADJF': 1, 'австрийский_ADJF': 1, 'грац_NOUN': 1, 'выяснить_VERB': 1, 'большинство_NOUN': 1, 'человек_NOUN': 1, 'неправильно_ADVB': 1, 'есть_VERB': 1, 'исследователь_NOUN': 1, 'утверждать_VERB': 1, 'до_PREP': 1, 'процент_NOUN': 1, 'полезный_ADJF': 1, 'вещество_NOUN': 1, 'сосредоточить_PRTS': 1, 'сердцевина_NOUN': 1, 'это_NPRO': 1, 'фрукт_NOUN': 1, 'поэтому_ADVB': 1, 'желательно_ADVB': 1, 'съедать_INFN': 1, 'вместе_ADVB': 1, 'с_PREP': 1, 'огрызок_NOUN': 1})\n",
      "\n",
      "{'учёный_NOUN': 2, 'из_PREP': 2, 'университет_NOUN': 2, 'и_CONJ': 4, 'эффект_NOUN': 2, 'яблоко_NOUN': 4, 'исследование_NOUN': 2, 'в_PREP': 5, 'на_PREP': 3, 'мозг_NOUN': 2, 'они_NPRO': 2, 'фитонутриент_NOUN': 3, 'что_CONJ': 5, 'тот_ADJF': 2}\n"
     ]
    }
   ],
   "source": [
    "words = normalizePymorphy(text)\n",
    "wdict = Counter(words) # Объект сразу посчитает частоты элементов списка.\n",
    "print(wdict)\n",
    "print()\n",
    "print({w:n for w,n in wdict.items() if n>1}) # Посмотрим какие слова встречаются больше одного раза."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Фактически, выше мы провели преобразование текста в вектор. Пространство вектора определено на словаре текста - количество измерений совпадает с количеством слов, каждому измерению сопоставлено какое-то слово и отложена его частота. Подобный подход называют мешком слов (Bag of Words, BoW), так как все слова перемешиваются, их порядок больше не соблюдается, а сами слова сваливаются в один \"мешок\".\n",
    "\n",
    "![](img/donkey_carrot_text.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание на распределение частот в отдельных словах и парах. Такое распределение называется [распределением Ципфа](https://ru.wikipedia.org/wiki/%D0%97%D0%B0%D0%BA%D0%BE%D0%BD_%D0%A6%D0%B8%D0%BF%D1%84%D0%B0) и является характерным практически для любого распределения частот слов и их комбинаций в текстах на любом естественном языке.\n",
    "\n",
    "$p_i=\\frac{p_0}{\\beta^{-\\alpha*i}}$, где $\\alpha\\approx1$.\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/ru/thumb/d/d8/WikipediaZipf20061023.png/450px-WikipediaZipf20061023.png)\n",
    "\n",
    "Но вообще, для расчета частот существует CountVectorizer, который позволяет сделать это всё за один раз и очень хорошо ложится в конвейер."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кстати, всё то же самое можно было посчитать при помощи класса [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) из библиотеки [skLearn](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_extraction.text). Но её мы оставим на самостоятельное изучение."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь обратите внимание на ещё пару фактов. Во-первых, наиболее частотными оказали служебные слова. Во-вторых, не все слова из аннотации, написанной человеком, вошли в список наиболее частотных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'учёный_NOUN': 2, 'из_PREP': 2, 'университет_NOUN': 2, 'и_CONJ': 4, 'эффект_NOUN': 2, 'яблоко_NOUN': 4, 'исследование_NOUN': 2, 'в_PREP': 5, 'на_PREP': 3, 'мозг_NOUN': 2, 'они_NPRO': 2, 'фитонутриент_NOUN': 3, 'что_CONJ': 5, 'тот_ADJF': 2}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Содержащиеся в этих плодах фитонутриенты способствуют образованию новых нейронов'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print({w:n for w,n in wdict.items() if n>1})\n",
    "description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем функцию, которая бы оставляла слова только со значимыми частями речи (краткое и полное прилагательное, существительное, глагол, краткая и полная формы причастия, деепричастие)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_POS = ['ADJF', 'ADJS', 'NOUN', 'VERB', 'PRTF', 'PRTS', 'GRND']\n",
    "\n",
    "def getMostFrequentWordsFiltered(text):\n",
    "    \n",
    "    tokens = re.findall('[A-Za-zА-Яа-яЁё]+\\-[A-Za-zА-Яа-яЁё]+|[A-Za-zА-Яа-яЁё]+', text)\n",
    "    words = []\n",
    "    for t in tokens:\n",
    "        pv = morph.parse(t)\n",
    "        if pv[0].tag.POS in imp_POS and pv[0].normal_form != 'быть':\n",
    "            words.append(pv[0].normal_form)\n",
    "    wdict = Counter(words)\n",
    "    wdict = {w:n for w,n in wdict.items() if n>1}\n",
    "    return sorted(wdict.items(), key = lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('яблоко', 4),\n",
       " ('учёный', 3),\n",
       " ('фитонутриент', 3),\n",
       " ('университет', 2),\n",
       " ('эффект', 2),\n",
       " ('исследование', 2),\n",
       " ('мозг', 2),\n",
       " ('тот', 2)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getMostFrequentWordsFiltered(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Представим себе, что мы взяли много текстов и для каждого из них посчитали вектор слов. Результатом работы является [разреженная матрица](https://docs.scipy.org/doc/scipy/reference/sparse.html) частот слов. Если мы возьмем большое количество текстов, то в кажом из них встречается не так много разных слов, но словарь всех текстов вместе будет огромен. Обработка текстов должна вестись в едином пространстве. Пусть это будет пространство словаря всех текстов (в противном случае у каждого текста будет свое собственное пространство, что чрезвычайно неудобно). Получается, что для текста с маленьким словарем мы должны хранить большое число нулей. Для того, чтобы этого избежать, хранят, например, один раз номер строки, индексы ненулевых значений и сами значения, то есть чуть больше двух чисел на ненулевое значение. Если считать, что словарь заметки - 100 слов, а словарь всех текстов - 100 000 слов, мы получаем экономию места в 500 раз. То, что считалось на кластере, теперь может считаться на недобуке с 2 Гб оперативной памяти.\n",
    "\n",
    "![](img/term-document-matrix-bow-annotated.png)\n",
    "Изображение взято [отсюда](https://livebook.manning.com/book/natural-language-processing-in-action/chapter-4/v-4/61)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь попробуем другой показатель для подсчета важности слов в тексте - $TF*IDF$. Здесь $TF$ - Term Frequency, частота термина в документе, а $IDF$ - Inverted Document Frequency, обратная частота термина в коллекции (количество документов, в которых встречается данный термин).\n",
    "\n",
    "Идея метрики очень проста. Если слово встречается почти во всех документах - его различительная сила очень мала и само слово не является важным. Если слово часто встречается в данном документе, то оно являетсяя важным для него.\n",
    "\n",
    "\n",
    "![](img/term-document-matrix-tfidf-annotated.png)\n",
    "Изображение взято [отсюда](https://livebook.manning.com/book/natural-language-processing-in-action/chapter-4/v-4/61).\n",
    "\n",
    "\n",
    "Метрика считается на коллекции документов для каждого слова, каждого документа. Для расчета меры можно использовать `TfidfVectorizer`, который работает так же как `CountVectorizer`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но нам необходимо искать новости, которые интересны пользователю.\n",
    "\n",
    "Для определения меры сходства двух статей теперь может использоваться косинусная мера сходства, рассчитываемая по следующей формуле: $cos(a,b)=\\frac{\\sum{a_i * b_i}}{\\sqrt {\\sum{a_i^2}*\\sum{b_i^2}}}$.\n",
    "\n",
    "Косинусная мера сходство смотрит не на близость конечных точек, а на то, в какую сторону направлены векторы.\n",
    "\n",
    "![](img/cosine_watch.jpg)\n",
    "\n",
    "Вообще-то, использовать стандартную функцию рассчета косинусной меры сходства из <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html\">sklearn</a> было бы быстрее. Но в данной задаче нам бы пришлось сводить все словари в один, чтобы на одних и тех же местах в векторе были частоты одних и тех же слов. Чтобы избежать подобной работы, напишем собственную функцию рассчета косинусного расстояния, работающую с разреженными векторами в виде питоновских словарей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь следует остановиться на том, что косинусная мера является мерой расстояния между объектами, причем подобных мер существует достаточно много. Обычно мы используем Евклидово расстояние: $d_E=\\sqrt{\\sum {(a_i-b_i)^2}}$. Но при обработке текстов оно работает гораздо хуже. Представим себе, что у нас есть два текста: текст статьи и текст статьи, объединенный с самим собой. Евклидово расстояние между ними будет значительным, тогда как содержание не изменится.\n",
    "\n",
    "Помимо Евклидового расстояния часто используется Манхэттенское расстояние (расстояние городских кварталов). Если взглянуть на карту Манхэттена, то мы увидим, что практически все улицы параллельны и перпендикулярны друг другу. Это означает, что выбирая любой не удлинняющий маршрут из одной точки в другую я пройду одно и то же расстояние: $d_M=\\sum {|a_i-b_i|}$. Манхэттенское расстояние используется в тех случаях, когда мы берем, например, взвешенную сумму параметров с тем, чтобы получить единую оценку. Например, оценивая различные офисы мы складываем с некоторыми коэффициентами стоимость, площадь, расстояние от центра или дома, оценку инфраструктуры и так далее. Аналогично можно брать разницу между двумя векторными представлениями офисов, чтобы найти насколько они сходны.\n",
    "\n",
    "Расстояние Жаккарда берет отношение размера пересечения словарей к их объединению: $d_J=\\frac{A \\cup B}{A \\cap B}$. Эта мера проверяет степень совпадения словарей двух текстов. Если словари совпадают полностью, то тексты, скорее всего, говорят об одном и том же.\n",
    "\n",
    "Однако, в одном тексте может обсуждаться производство шестеренок, а во вводной части однажды будет упомянуто, что они необходимы для сбора механизмов, тогда как в другом тексте будут обсуждаться сами механизмы с единственным упоминанием, что они состоят из шестеренок. Случайное появление отдельных слов сделает тексты более похожими, чем это следует из их содержания. Этот недостаток устраняет косинусная мера сходства: $d_{cos}=\\frac{\\sum{a_i * b_i}}{\\sqrt {\\sum{a_i^2}*\\sum{b_i^2}}}$. Если в одном из текстов не встречается слово из другого текста, то соответствующий член суммы в числителе будет равен нулю. Если в одном тексте слово встречается часто, а в другом редко, произведение не будет слишком большим. Проблему представляет случай, когда в обоих текстах есть несколько очень часто встречающихся слов. Тогда их произведение будет забивать все остальные слова, искажая общий смысл.\n",
    "\n",
    "Следует иметь в виду, что косинусная мера сходства является величиной, обратно зависящей от расстояния: два одинаковых текста будут иметь косинусное сходство равное 1, тогда как расстояние между ними равно нулю, и наоборот.\n",
    "\n",
    "В качестве мер расстояния также используются [корреляция](https://ru.wikipedia.org/wiki/%D0%9A%D0%BE%D1%80%D1%80%D0%B5%D0%BB%D1%8F%D1%86%D0%B8%D1%8F) и [дивергенция Кулльбака-Лейблера](https://ru.wikipedia.org/wiki/%D0%A0%D0%B0%D1%81%D1%81%D1%82%D0%BE%D1%8F%D0%BD%D0%B8%D0%B5_%D0%9A%D1%83%D0%BB%D1%8C%D0%B1%D0%B0%D0%BA%D0%B0_%E2%80%94_%D0%9B%D0%B5%D0%B9%D0%B1%D0%BB%D0%B5%D1%80%D0%B0).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
